{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST in Keras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "pMgPT-4QqBMN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Building a simple feed forward or convolutional2D neural-network with Keras\n",
        "\n",
        "**Author: Xavier Snelgrove**\n",
        "\n",
        "This is a simple quick-start in performing digit recognition in a neural network in Keras, for a short tutorial at the University of Toronto.\n",
        "\n",
        "It is largely based on the `mnist_mlp.py` example from the Keras source.\n",
        "\n",
        "Source: https://github.com/wxs/keras-mnist-tutorial.git\n"
      ]
    },
    {
      "metadata": {
        "id": "D3nvMoKfG9hu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Loading the data and setting up the environment"
      ]
    },
    {
      "metadata": {
        "id": "TlyI9u67qBMR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Let's start!\n",
        "\n",
        "Specify some details for running the network:\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "eLZ6vfnO-jUD",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "#model_flag = 'FullyConnected' # Choose between 'FullyConnected' or 'Conv2D'\n",
        "model_flag = 'Conv2D' # Choose between 'FullyConnected' or 'Conv2D'\n",
        "epochs = 4 # Number of epochs to train the network\n",
        "batch_size = 128 # Number of samples used for one weight update\n",
        "if model_flag not in ['FullyConnected', 'Conv2D']:\n",
        "  raise ValueError('wrong model chosen: %s'%model_flag)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f7BR15op-iG5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First let's import some libraries and set the environment"
      ]
    },
    {
      "metadata": {
        "id": "YKyhPAoUqBMS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = (9,9) # Make the figures a bit bigger\n",
        "\n",
        "from keras.datasets import mnist #mnist dataset (and many more) are available online\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "try:\n",
        "  import pydot\n",
        "  from keras_sequential_ascii import sequential_model_to_ascii_printout\n",
        "except ImportError:\n",
        "  print('>>>> Restart from the top because pydot was just installed <<<<')\n",
        "  !pip install -q keras_sequential_ascii\n",
        "  !apt-get -qq install -y graphviz && pip install -q pydot >/dev/null 2>&1\n",
        "  import os, signal\n",
        "  os.kill(os.getpid(), signal.SIGKILL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_qR_uXSuqBMZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load training data"
      ]
    },
    {
      "metadata": {
        "id": "ZPuKFMi-qBMa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nb_classes = 10 # number of classes. Here digits 0-9, so 10 classes\n",
        "\n",
        "# the data, shuffled and split between tran and test sets\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "print(\"X_train original shape\", X_train.shape)\n",
        "print(\"y_train original shape\", y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wmBvgPSrqBMg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's look at some examples of the training data"
      ]
    },
    {
      "metadata": {
        "id": "_5vrtO1GqBMh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "for i in range(9):\n",
        "    plt.subplot(3,3,i+1)\n",
        "    plt.imshow(X_train[i], cmap='gray', interpolation='none')\n",
        "    plt.title(\"Class {}\".format(y_train[i]))\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tQ0G3LnDqBMm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Format the data for training\n",
        "We need to reshape the input tensor (image) so that it fits the neural network.\n",
        "\n",
        "The fully connected neural network is going to take a single vector for each training example, so we need to reshape the input so that each 28x28 image becomes a single 784 dimensional vector. \n",
        "\n",
        "The convolutional neural network is going to take the 28x28 image for each training example, but we need to add an 'color' axis. In our case (grayscale) its length is 1 whereas for a RGB image it would have a length of 3.\n",
        "\n",
        "We'll also scale the inputs to be in the range [0-1] rather than [0-255]"
      ]
    },
    {
      "metadata": {
        "id": "3_thpI4vqBMo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if model_flag == 'FullyConnected':\n",
        "  X_train = X_train.reshape(60000, 784)\n",
        "  X_test = X_test.reshape(10000, 784)\n",
        "elif model_flag == 'Conv2D':\n",
        "  X_train = X_train[...,np.newaxis]\n",
        "  X_test = X_test[...,np.newaxis]\n",
        "else: raise ValueError('wrong model chosen: %s'%model_flag)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "print(\"Training matrix shape\", X_train.shape)\n",
        "print(\"Testing matrix shape\", X_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZuV51tpxqBMu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The classifier's output expects matrices to be in the one-hot format, i.e.\n",
        "\n",
        "```\n",
        "0 -> [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "1 -> [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
        "2 -> [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
        "etc.\n",
        "9 -> [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "AToAeV2XqBMv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, nb_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uGq1rj2HqBMz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Build the neural network\n",
        "Build the neural-network. Here we'll do a simple layer fully connected network or a simple convolutional neural network (decision was made above)\n",
        "Below we visualize the network architecture in different ways"
      ]
    },
    {
      "metadata": {
        "id": "JZS62nbZqBM1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fully_connected_model():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(64, input_shape=(784,)))  #Fully connected layer with 64 neurons\n",
        "                                            #Only the 1st layer need to know the input shape\n",
        "  model.add(Activation('relu')) # An \"activation\" is just a non-linear function applied to the output\n",
        "                                # of the layer above. Here, with a \"rectified linear unit\", meaning f(x)=max(0,x)\n",
        "\n",
        "  model.add(Dropout(0.2))   # Dropout helps protect the model from memorizing or \"overfitting\" the training data\n",
        "                            # by disconnecting a fraction (here 20%) of random connections during training\n",
        "  model.add(Dense(128))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(10))\n",
        "  model.add(Activation('softmax')) # This special \"softmax\" activation among other things,\n",
        "                                   # ensures the output is a valid probaility distribution, that is\n",
        "                                   # that its values are all non-negative and sum up to 1.\n",
        "  return model\n",
        "\n",
        "def convolutional2D_model():\n",
        "  model = Sequential()\n",
        "\n",
        "  # CONVOLUTION PART\n",
        "  model.add(Conv2D(16, kernel_size=(3, 3), input_shape=(28, 28, 1)))  #Convolution2D layer with a kernel size (receptive field) of 3x3 pixel\n",
        "                                                                      #and 16 kernel, resulting in 16 different feature maps  \n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2))) #Pooling layer that decreases the dimension of the layers by\n",
        "                                             #combining the outputs of neuron clusters (here 2x2) at one layer \n",
        "                                             #into a single neuron in the next layer that receives \n",
        "                                             #the maximum value of the outputs cluster.\n",
        "  model.add(Dropout(0.2))\n",
        "  \n",
        "  model.add(Conv2D(32, kernel_size=(3, 3)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "\n",
        "  # REGRESSION PART\n",
        "  model.add(Flatten()) #(N x N x 32) is flattened to by a 1D vector of length N*N*32\n",
        "  model.add(Dense(20))\n",
        "  model.add(Dense(10))\n",
        "  model.add(Activation('softmax'))\n",
        "  \n",
        "  return model\n",
        "\n",
        "if model_flag == 'FullyConnected':\n",
        "  model = fully_connected_model()\n",
        "elif model_flag == 'Conv2D':\n",
        "  model = convolutional2D_model()\n",
        "else: raise ValueError('wrong model chosen: %s'%model_flag)\n",
        "    \n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ox2ArU_cxlZC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import plot_model\n",
        "from IPython.display import Image\n",
        "plot_model(model, to_file='plot_model.png', show_shapes=True, show_layer_names=True)\n",
        "Image('plot_model.png', height=600)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Ls8ZDVXhdwf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sequential_model_to_ascii_printout(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "882pzpP8qBM5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Compile the model\n",
        "Keras is built on top of  the TensorFlow library (and Theano as well), both packages that allow you to define a *computation graph* in Python, which they then compile and run efficiently on the CPU or GPU without the overhead of the Python interpreter.\n",
        "\n",
        "When compiling a model, Keras asks you to specify your **loss function** and your **optimizer** and optionally a **metric**. The loss function is the function that is minimized during training by an suitable algorithm given by the optimizer. The loss function we'll use here is called *categorical crossentropy*, and is a loss function well-suited to comparing two probability distributions. The metric is similar to the loss function but this value is just monitorred and not minimized. The optimizer helps determine how quickly the model learns, how resistent it is to getting \"stuck\" or \"blowing up\". We won't discuss this in too much detail, but \"adam\" is often a good choice.\n",
        "\n",
        "Here our predictions are probability distributions across the ten different digits (e.g. \"we're 80% confident this image is a 3, 10% sure it's an 8, 5% it's a 2, etc.\"), and the target is a probability distribution with 100% for the correct category, and 0% for everything else. The cross-entropy is a measure of how different your predicted distribution is from the target distribution. [More detail at Wikipedia](https://en.wikipedia.org/wiki/Cross_entropy)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "NYVQ9jtPqBM6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xbmyObkJqBM-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train the model!\n",
        "This is the fun part: you can feed the training data loaded above into this model and it will learn to classify digits"
      ]
    },
    {
      "metadata": {
        "id": "XyiDDM_8qBM_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, Y_train,\n",
        "                    batch_size=batch_size, epochs=epochs,\n",
        "                    verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rHkW9AyAP4p5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Finally, evaluate the model performance\n",
        "The model is trained. Let's check the final loss and accuracy on test sample of course!\n",
        "Then, we look into the training curve (loss) and the accuracy curve.\n",
        "This information is directly provided by the training method."
      ]
    },
    {
      "metadata": {
        "id": "HeSvUpsiRJip",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zDePd7jm2mJI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochs = np.arange(1,len(history.history['acc'])+1)\n",
        "\n",
        "plt.figure()\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(epochs, history.history['loss'], label='Training')\n",
        "plt.plot(epochs, history.history['val_loss'], label='Validation')\n",
        "plt.xlabel('After Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='center right')\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(epochs, history.history['acc'], label='Training')\n",
        "plt.plot(epochs, history.history['val_acc'], label='Validation')\n",
        "plt.xlabel('After Epoch')\n",
        "plt.ylabel('Accuracy [%]')\n",
        "plt.legend(loc='center right')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jM-NSxxjqBNJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Inspecting the output\n",
        "\n",
        "It's always a good idea to inspect the output and make sure everything looks sane. Here we'll look at some examples it gets right, and some examples it gets wrong."
      ]
    },
    {
      "metadata": {
        "id": "zROqq1cqqBNK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The predict_classes function outputs the highest probability class\n",
        "# according to the trained classifier for each input example.\n",
        "predicted_classes = model.predict_classes(X_test)\n",
        "\n",
        "# Check which items we got right / wrong\n",
        "correct_indices = np.nonzero(predicted_classes == y_test)[0]\n",
        "incorrect_indices = np.nonzero(predicted_classes != y_test)[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4sHWBnYfqBNP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Let's plot some CORRECT samples\n",
        "plt.figure()\n",
        "for i, correct in enumerate(correct_indices[:9]):\n",
        "    plt.subplot(3,3,i+1)\n",
        "    plt.imshow(X_test[correct].reshape(28,28), cmap='gray', interpolation='none')\n",
        "    plt.title(\"Predicted {}, True Class {}\".format(predicted_classes[correct], y_test[correct]))\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SFNeC7Oc2DfI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Let's plot some WRONG samples\n",
        "plt.figure()\n",
        "for i, incorrect in enumerate(incorrect_indices[:9]):\n",
        "    plt.subplot(3,3,i+1)\n",
        "    plt.imshow(X_test[incorrect].reshape(28,28), cmap='gray', interpolation='none')\n",
        "    plt.title(\"Predicted {}, True Class {}\".format(predicted_classes[incorrect], y_test[incorrect]))\n",
        "    plt.axis('off') \n",
        "plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I1cwioWsqBNU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# That's all!"
      ]
    },
    {
      "metadata": {
        "id": "ubg1g4WyqBNV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There are lots of other great examples at the Keras homepage at http://keras.io and in the source code at https://github.com/fchollet/keras"
      ]
    }
  ]
}